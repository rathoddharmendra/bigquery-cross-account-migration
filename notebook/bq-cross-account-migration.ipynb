{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7730bd0-c776-46de-a9b8-33bda8df9818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-auth in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.40.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from google-auth) (5.5.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from google-auth) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from google-auth) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from rsa<5,>=3.1.4->google-auth) (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "# install requirements (one-time)\n",
    "#!pip install google-cloud-bigquery google-cloud-storage tqdm\n",
    "!pip install google-auth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fffe115-e4fe-4a0b-a718-ff74c0707300",
   "metadata": {},
   "source": [
    "## Read Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c179db0b-2475-4748-b2ca-93ea1f92bf37",
   "metadata": {},
   "source": [
    "## Define Imports and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3bdc9aa-4bc8-4cd5-8870-cbae71231cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from google.api_core.exceptions import NotFound, Forbidden, GoogleAPICallError\n",
    "from tqdm import tqdm\n",
    "from google.api_core.exceptions import Conflict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68f13c57-da06-49be-a3f7-5c90ebdc21cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates new name with 2 number suffix\n",
    "def _make_random_suffix(n=2):\n",
    "    return ''.join(random.choices('0123456789', k=n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4361b7b-ff77-4b3d-8594-208fbd5a9129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_source_client_from_key(key_path: str) -> bigquery.Client:\n",
    "    creds = service_account.Credentials.from_service_account_file(key_path)\n",
    "    return bigquery.Client(credentials=creds, project=creds.project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef28d45c-a82b-46a7-ba3f-5620b8339e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_creds = service_account.Credentials.from_service_account_file(\"../secrets/darry-r26.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3402777-43e7-41a6-bb99-7bc0f75ff649",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_client = bigquery.Client(credentials=src_creds, project=src_creds.project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dafa5811-c77a-4f92-a9b6-7d9f7807bbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = []\n",
    "for datasets in src_client.list_datasets():\n",
    "    # save under a plan\n",
    "    dataset_names.append(f'{datasets.project}.{datasets.dataset_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccfd0666-a4b8-4f6e-9a3b-4e96a663d03d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e6f3b42-f82d-4be1-84cb-2f5c790fbaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data-analytics-ns\n"
     ]
    }
   ],
   "source": [
    "for projects in src_client.list_projects():\n",
    "    print(projects.project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5511ae61-1aac-4be9-a09e-b0c6efdc21a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data-analytics-ns:course15.cc_funnel\n",
      "data-analytics-ns:course15.cc_funnel_kpi\n",
      "data-analytics-ns:course15.cc_funnel_local\n",
      "data-analytics-ns:course15.cc_parcel\n",
      "data-analytics-ns:course15.cc_parcel_kpi\n",
      "data-analytics-ns:course15.cc_parcel_kpi_formatted\n",
      "data-analytics-ns:course15.cc_parcel_kpi_global\n",
      "data-analytics-ns:course15.cc_parcel_kpi_month\n",
      "data-analytics-ns:course15.cc_parcel_kpi_priority\n",
      "data-analytics-ns:course15.cc_parcel_kpi_priority_by_ratio\n",
      "data-analytics-ns:course15.cc_parcel_kpi_transporter\n",
      "data-analytics-ns:course15.cc_parcel_product\n",
      "data-analytics-ns:course15.circle_sales\n",
      "data-analytics-ns:course15.circle_sales_daily\n",
      "data-analytics-ns:course15.circle_stock\n",
      "data-analytics-ns:course15.circle_stock_cat\n",
      "data-analytics-ns:course15.circle_stock_copy\n",
      "data-analytics-ns:course15.circle_stock_d7_ch2\n",
      "data-analytics-ns:course15.circle_stock_kpi\n",
      "data-analytics-ns:course15.circle_stock_kpi_top\n",
      "data-analytics-ns:course15.circle_stock_model_type\n",
      "data-analytics-ns:course15.circle_stock_name\n",
      "data-analytics-ns:course15.circle_stock_name_view\n",
      "data-analytics-ns:course15.d11_circle_stock_cat\n",
      "data-analytics-ns:course15.d11_circle_stock_kpi_view\n",
      "data-analytics-ns:course15.d11_circle_stock_name_view\n",
      "data-analytics-ns:course15.d9_cc_parcel\n",
      "data-analytics-ns:course15.d9_cc_parcel_aggfinal\n",
      "data-analytics-ns:course15.d9_cc_parcel_join\n",
      "data-analytics-ns:course15.d9_cc_parcel_product\n",
      "data-analytics-ns:course15.d9_cc_sales\n",
      "data-analytics-ns:course15.d9_cc_sales_aggfinal\n",
      "data-analytics-ns:course15.d9_cc_stock\n",
      "data-analytics-ns:course15.gwz_orders_17\n",
      "data-analytics-ns:course15.gwz_sales\n",
      "data-analytics-ns:course15.gwz_sales_category\n",
      "data-analytics-ns:course15.stock_updated\n",
      "data-analytics-ns:course15.top_products\n",
      "data-analytics-ns:course15.view_circle_stock\n"
     ]
    }
   ],
   "source": [
    "# for dataset in dataset_names:\n",
    "for table in src_client.list_tables('data-analytics-ns.course15'):\n",
    "    print(table.full_table_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "971329c6-f7ee-4761-98ef-c65c958c6985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table.RowIterator at 0x10f431940>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_client.query_and_wait('select * from data-analytics-ns.course15.cc_parcel_kpi_global')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6f1887-f570-4fdb-9cf7-22a5b157c5e5",
   "metadata": {},
   "source": [
    "## Destination Client Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2a7c50a-5b35-4c99-8d4f-34aa65eedaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created dataset data-analytics-ns-470609.test01_db\n"
     ]
    }
   ],
   "source": [
    "dst_creds = service_account.Credentials.from_service_account_file(\"../secrets/storiesmitdee.json\")\n",
    "dst_client = bigquery.Client(credentials=dst_creds, project=dst_creds.project_id)\n",
    "\n",
    "# Define dataset\n",
    "dataset_name = f'{dst_client.project}.test01_db'\n",
    "dataset = bigquery.Dataset(dataset_name)\n",
    "dataset.location = \"EU\"  # required, matches your migration region\n",
    "\n",
    "# Create dataset (if not exists)\n",
    "try:\n",
    "    dataset = dst_client.create_dataset(dataset)  # API request\n",
    "    print(f\"✅ Created dataset {dataset_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Dataset {dataset_name} already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "396c1a4f-4ee6-4805-9a52-2b5f264f08a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequest",
     "evalue": "400 data-analytics-ns:course15.cc_funnel is not allowed for this operation because it is currently a EXTERNAL.; reason: invalid, message: data-analytics-ns:course15.cc_funnel is not allowed for this operation because it is currently a EXTERNAL.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequest\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m job = dst_client.copy_table(src_table_id, dst_table_id, location=\u001b[33m\"\u001b[39m\u001b[33mEU\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Wait for job to complete\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mjob\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Copied table \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc_table_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m → \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdst_table_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/google/cloud/bigquery/job/base.py:1001\u001b[39m, in \u001b[36m_AsyncJob.result\u001b[39m\u001b[34m(self, retry, timeout)\u001b[39m\n\u001b[32m    998\u001b[39m     \u001b[38;5;28mself\u001b[39m._begin(retry=retry, timeout=timeout)\n\u001b[32m   1000\u001b[39m kwargs = {} \u001b[38;5;28;01mif\u001b[39;00m retry \u001b[38;5;129;01mis\u001b[39;00m DEFAULT_RETRY \u001b[38;5;28;01melse\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mretry\u001b[39m\u001b[33m\"\u001b[39m: retry}\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_AsyncJob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/google/api_core/future/polling.py:261\u001b[39m, in \u001b[36mPollingFuture.result\u001b[39m\u001b[34m(self, timeout, retry, polling)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._blocking_poll(timeout=timeout, retry=retry, polling=polling)\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# pylint: disable=raising-bad-type\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;66;03m# Pylint doesn't recognize that this is valid in this case.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[31mBadRequest\u001b[39m: 400 data-analytics-ns:course15.cc_funnel is not allowed for this operation because it is currently a EXTERNAL.; reason: invalid, message: data-analytics-ns:course15.cc_funnel is not allowed for this operation because it is currently a EXTERNAL."
     ]
    }
   ],
   "source": [
    "src_table_id = f\"{src_client.project}.course15.cc_funnel\"\n",
    "dst_table_id = f\"{dst_client.project}.test01_db.my_table_copy\"\n",
    "\n",
    "# Configure the copy job\n",
    "job = dst_client.copy_table(src_table_id, dst_table_id, location=\"EU\")\n",
    "\n",
    "# Wait for job to complete\n",
    "job.result()\n",
    "print(f\"✅ Copied table {src_table_id} → {dst_table_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36fb01b3-2a59-4d9a-a72b-7565e15e8c81",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Please install the 'db-dtypes' package to use this function.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/google/cloud/bigquery/_pandas_helpers.py:59\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdb_dtypes\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     61\u001b[39m     date_dtype_name = db_dtypes.DateDtype.name\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'db_dtypes'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m rows = src_client.list_rows(src_table_id)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mrows\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# Pandas dataframe with data\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/google/cloud/bigquery/table.py:2597\u001b[39m, in \u001b[36mRowIterator.to_dataframe\u001b[39m\u001b[34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, geography_as_object, bool_dtype, int_dtype, float_dtype, string_dtype, date_dtype, datetime_dtype, time_dtype, timestamp_dtype, range_date_dtype, range_datetime_dtype, range_timestamp_dtype)\u001b[39m\n\u001b[32m   2362\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mto_dataframe\u001b[39m(\n\u001b[32m   2363\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2364\u001b[39m     bqstorage_client: Optional[\u001b[33m\"\u001b[39m\u001b[33mbigquery_storage.BigQueryReadClient\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2383\u001b[39m     ] = DefaultPandasDTypes.RANGE_TIMESTAMP_DTYPE,\n\u001b[32m   2384\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mpandas.DataFrame\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2385\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create a pandas DataFrame by loading all pages of a query.\u001b[39;00m\n\u001b[32m   2386\u001b[39m \n\u001b[32m   2387\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2595\u001b[39m \n\u001b[32m   2596\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2597\u001b[39m     \u001b[43m_pandas_helpers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mverify_pandas_imports\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2599\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m geography_as_object \u001b[38;5;129;01mand\u001b[39;00m shapely \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2600\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(_NO_SHAPELY_ERROR)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/google/cloud/bigquery/_pandas_helpers.py:1117\u001b[39m, in \u001b[36mverify_pandas_imports\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1115\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(_NO_PANDAS_ERROR) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas_import_exception\u001b[39;00m\n\u001b[32m   1116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m db_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(_NO_DB_TYPES_ERROR) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdb_dtypes_import_exception\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Please install the 'db-dtypes' package to use this function."
     ]
    }
   ],
   "source": [
    "rows = src_client.list_rows(src_table_id)\n",
    "df = rows.to_dataframe()   # Pandas dataframe with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e7cbbc3-106b-412c-8a0d-4f876d6e11ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFound",
     "evalue": "404 Not found: Table data-analytics-ns:course15.cc_funnel_native_native was not found in location EU; reason: notFound, message: Not found: Table data-analytics-ns:course15.cc_funnel_native_native was not found in location EU\n\nLocation: EU\nJob ID: 84b3c6c0-389b-4c29-bc42-f448bbdb2a65\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFound\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m      9\u001b[39m     src_client.query(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCREATE OR REPLACE TABLE `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc_table_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_native` AS SELECT * FROM `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc_table_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     df = \u001b[43msrc_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSELECT * FROM `\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msrc_table_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_native`\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Read \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows from source table\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# 2️⃣ Load DataFrame into destination table\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/google/cloud/bigquery/job/query.py:2092\u001b[39m, in \u001b[36mQueryJob.to_dataframe\u001b[39m\u001b[34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, max_results, geography_as_object, bool_dtype, int_dtype, float_dtype, string_dtype, date_dtype, datetime_dtype, time_dtype, timestamp_dtype, range_date_dtype, range_datetime_dtype, range_timestamp_dtype)\u001b[39m\n\u001b[32m   1862\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mto_dataframe\u001b[39m(\n\u001b[32m   1863\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1864\u001b[39m     bqstorage_client: Optional[\u001b[33m\"\u001b[39m\u001b[33mbigquery_storage.BigQueryReadClient\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1884\u001b[39m     ] = DefaultPandasDTypes.RANGE_TIMESTAMP_DTYPE,\n\u001b[32m   1885\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mpandas.DataFrame\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1886\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a pandas DataFrame from a QueryJob\u001b[39;00m\n\u001b[32m   1887\u001b[39m \n\u001b[32m   1888\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2090\u001b[39m \u001b[33;03m            :mod:`shapely` library cannot be imported.\u001b[39;00m\n\u001b[32m   2091\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2092\u001b[39m     query_result = \u001b[43mwait_for_query\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2093\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m query_result.to_dataframe(\n\u001b[32m   2094\u001b[39m         bqstorage_client=bqstorage_client,\n\u001b[32m   2095\u001b[39m         dtypes=dtypes,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2109\u001b[39m         range_timestamp_dtype=range_timestamp_dtype,\n\u001b[32m   2110\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/google/cloud/bigquery/_tqdm_helpers.py:107\u001b[39m, in \u001b[36mwait_for_query\u001b[39m\u001b[34m(query_job, progress_bar_type, max_results)\u001b[39m\n\u001b[32m    103\u001b[39m progress_bar = get_progress_bar(\n\u001b[32m    104\u001b[39m     progress_bar_type, \u001b[33m\"\u001b[39m\u001b[33mQuery is running\u001b[39m\u001b[33m\"\u001b[39m, default_total, \u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    105\u001b[39m )\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m progress_bar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mquery_job\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m i = \u001b[32m0\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/google/cloud/bigquery/job/query.py:1706\u001b[39m, in \u001b[36mQueryJob.result\u001b[39m\u001b[34m(self, page_size, max_results, retry, timeout, start_index, job_retry)\u001b[39m\n\u001b[32m   1701\u001b[39m     remaining_timeout = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining_timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1704\u001b[39m     \u001b[38;5;66;03m# Since is_job_done() calls jobs.getQueryResults, which is a\u001b[39;00m\n\u001b[32m   1705\u001b[39m     \u001b[38;5;66;03m# long-running API, don't delay the next request at all.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1706\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_job_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1707\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   1708\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1709\u001b[39m     \u001b[38;5;66;03m# Use a monotonic clock since we don't actually care about\u001b[39;00m\n\u001b[32m   1710\u001b[39m     \u001b[38;5;66;03m# daylight savings or similar, just the elapsed time.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[32m    167\u001b[39m     time.sleep(next_sleep)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/google/api_core/retry/retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    216\u001b[39m     on_error_fn(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    149\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/google/cloud/bigquery/job/query.py:1655\u001b[39m, in \u001b[36mQueryJob.result.<locals>.is_job_done\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1632\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m job_failed_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1633\u001b[39m     \u001b[38;5;66;03m# Only try to restart the query job if the job failed for\u001b[39;00m\n\u001b[32m   1634\u001b[39m     \u001b[38;5;66;03m# a retriable reason. For example, don't restart the query\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1652\u001b[39m     \u001b[38;5;66;03m# into an exception that can be processed by the\u001b[39;00m\n\u001b[32m   1653\u001b[39m     \u001b[38;5;66;03m# `job_retry` predicate.\u001b[39;00m\n\u001b[32m   1654\u001b[39m     restart_query_job = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1655\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m job_failed_exception\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1657\u001b[39m     \u001b[38;5;66;03m# Make sure that the _query_results are cached so we\u001b[39;00m\n\u001b[32m   1658\u001b[39m     \u001b[38;5;66;03m# can return a complete RowIterator.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1664\u001b[39m     \u001b[38;5;66;03m# making any extra API calls if the previous loop\u001b[39;00m\n\u001b[32m   1665\u001b[39m     \u001b[38;5;66;03m# iteration fetched the finished job.\u001b[39;00m\n\u001b[32m   1666\u001b[39m     \u001b[38;5;28mself\u001b[39m._reload_query_results(\n\u001b[32m   1667\u001b[39m         retry=retry, **reload_query_results_kwargs\n\u001b[32m   1668\u001b[39m     )\n",
      "\u001b[31mNotFound\u001b[39m: 404 Not found: Table data-analytics-ns:course15.cc_funnel_native_native was not found in location EU; reason: notFound, message: Not found: Table data-analytics-ns:course15.cc_funnel_native_native was not found in location EU\n\nLocation: EU\nJob ID: 84b3c6c0-389b-4c29-bc42-f448bbdb2a65\n"
     ]
    }
   ],
   "source": [
    "# Source & destination\n",
    "src_table_id = f\"{src_client.project}.course15.cc_funnel_native\"\n",
    "dst_table_id = f\"{dst_client.project}.test01_db.cc_funnel_copy\"\n",
    "\n",
    "# 1️⃣ Read source table into DataFrame\n",
    "try:\n",
    "    df = src_client.query(f\"SELECT * FROM `{src_table_id}`\").to_dataframe()\n",
    "except:\n",
    "    src_client.query(f\"CREATE OR REPLACE TABLE `{src_table_id}_native` AS SELECT * FROM `{src_table_id}`\")\n",
    "    df = src_client.query(f\"SELECT * FROM `{src_table_id}_native`\").to_dataframe()\n",
    "\n",
    "\n",
    "print(f\"✅ Read {len(df)} rows from source table\")\n",
    "\n",
    "# 2️⃣ Load DataFrame into destination table\n",
    "job = dst_client.load_table_from_dataframe(df, dst_table_id)\n",
    "job.result()\n",
    "print(f\"✅ Loaded {len(df)} rows into destination table {dst_table_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69fc1bcd-65d0-45cb-92a8-b1ebd353d4c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "Forbidden",
     "evalue": "403 Access Denied: BigQuery BigQuery: Permission denied while getting Drive credentials.; reason: accessDenied, location: 1YJOR3XYHU3-KaxvI95bDgmVSeUjDXrEsTJnbOGYftE8, message: Access Denied: BigQuery BigQuery: Permission denied while getting Drive credentials.\n\nLocation: EU\nJob ID: cd923b73-17ce-4396-9a43-6e1fed8e9413\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mForbidden\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43msrc_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSELECT * FROM `\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msrc_table_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m`\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/google/cloud/bigquery/job/query.py:2092\u001b[39m, in \u001b[36mQueryJob.to_dataframe\u001b[39m\u001b[34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, max_results, geography_as_object, bool_dtype, int_dtype, float_dtype, string_dtype, date_dtype, datetime_dtype, time_dtype, timestamp_dtype, range_date_dtype, range_datetime_dtype, range_timestamp_dtype)\u001b[39m\n\u001b[32m   1862\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mto_dataframe\u001b[39m(\n\u001b[32m   1863\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1864\u001b[39m     bqstorage_client: Optional[\u001b[33m\"\u001b[39m\u001b[33mbigquery_storage.BigQueryReadClient\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1884\u001b[39m     ] = DefaultPandasDTypes.RANGE_TIMESTAMP_DTYPE,\n\u001b[32m   1885\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mpandas.DataFrame\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1886\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a pandas DataFrame from a QueryJob\u001b[39;00m\n\u001b[32m   1887\u001b[39m \n\u001b[32m   1888\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2090\u001b[39m \u001b[33;03m            :mod:`shapely` library cannot be imported.\u001b[39;00m\n\u001b[32m   2091\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2092\u001b[39m     query_result = \u001b[43mwait_for_query\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2093\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m query_result.to_dataframe(\n\u001b[32m   2094\u001b[39m         bqstorage_client=bqstorage_client,\n\u001b[32m   2095\u001b[39m         dtypes=dtypes,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2109\u001b[39m         range_timestamp_dtype=range_timestamp_dtype,\n\u001b[32m   2110\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/google/cloud/bigquery/_tqdm_helpers.py:107\u001b[39m, in \u001b[36mwait_for_query\u001b[39m\u001b[34m(query_job, progress_bar_type, max_results)\u001b[39m\n\u001b[32m    103\u001b[39m progress_bar = get_progress_bar(\n\u001b[32m    104\u001b[39m     progress_bar_type, \u001b[33m\"\u001b[39m\u001b[33mQuery is running\u001b[39m\u001b[33m\"\u001b[39m, default_total, \u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    105\u001b[39m )\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m progress_bar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mquery_job\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m i = \u001b[32m0\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/google/cloud/bigquery/job/query.py:1706\u001b[39m, in \u001b[36mQueryJob.result\u001b[39m\u001b[34m(self, page_size, max_results, retry, timeout, start_index, job_retry)\u001b[39m\n\u001b[32m   1701\u001b[39m     remaining_timeout = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining_timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1704\u001b[39m     \u001b[38;5;66;03m# Since is_job_done() calls jobs.getQueryResults, which is a\u001b[39;00m\n\u001b[32m   1705\u001b[39m     \u001b[38;5;66;03m# long-running API, don't delay the next request at all.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1706\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_job_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1707\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   1708\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1709\u001b[39m     \u001b[38;5;66;03m# Use a monotonic clock since we don't actually care about\u001b[39;00m\n\u001b[32m   1710\u001b[39m     \u001b[38;5;66;03m# daylight savings or similar, just the elapsed time.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[32m    167\u001b[39m     time.sleep(next_sleep)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/google/api_core/retry/retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    216\u001b[39m     on_error_fn(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    149\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/google/cloud/bigquery/job/query.py:1655\u001b[39m, in \u001b[36mQueryJob.result.<locals>.is_job_done\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1632\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m job_failed_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1633\u001b[39m     \u001b[38;5;66;03m# Only try to restart the query job if the job failed for\u001b[39;00m\n\u001b[32m   1634\u001b[39m     \u001b[38;5;66;03m# a retriable reason. For example, don't restart the query\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1652\u001b[39m     \u001b[38;5;66;03m# into an exception that can be processed by the\u001b[39;00m\n\u001b[32m   1653\u001b[39m     \u001b[38;5;66;03m# `job_retry` predicate.\u001b[39;00m\n\u001b[32m   1654\u001b[39m     restart_query_job = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1655\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m job_failed_exception\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1657\u001b[39m     \u001b[38;5;66;03m# Make sure that the _query_results are cached so we\u001b[39;00m\n\u001b[32m   1658\u001b[39m     \u001b[38;5;66;03m# can return a complete RowIterator.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1664\u001b[39m     \u001b[38;5;66;03m# making any extra API calls if the previous loop\u001b[39;00m\n\u001b[32m   1665\u001b[39m     \u001b[38;5;66;03m# iteration fetched the finished job.\u001b[39;00m\n\u001b[32m   1666\u001b[39m     \u001b[38;5;28mself\u001b[39m._reload_query_results(\n\u001b[32m   1667\u001b[39m         retry=retry, **reload_query_results_kwargs\n\u001b[32m   1668\u001b[39m     )\n",
      "\u001b[31mForbidden\u001b[39m: 403 Access Denied: BigQuery BigQuery: Permission denied while getting Drive credentials.; reason: accessDenied, location: 1YJOR3XYHU3-KaxvI95bDgmVSeUjDXrEsTJnbOGYftE8, message: Access Denied: BigQuery BigQuery: Permission denied while getting Drive credentials.\n\nLocation: EU\nJob ID: cd923b73-17ce-4396-9a43-6e1fed8e9413\n"
     ]
    }
   ],
   "source": [
    "df = src_client.query(f\"SELECT * FROM `{src_table_id}`\").to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23737864-6919-4d20-a398-616b6a5d8a69",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m auth\n\u001b[32m      2\u001b[39m auth.authenticate_user()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4592adc2-dd77-4100-b70a-1770d23c22f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External table blocked, creating native copy... 403 Access Denied: BigQuery BigQuery: Permission denied while getting Drive credentials.; reason: accessDenied, location: 1YJOR3XYHU3-KaxvI95bDgmVSeUjDXrEsTJnbOGYftE8, message: Access Denied: BigQuery BigQuery: Permission denied while getting Drive credentials.\n",
      "\n",
      "Location: EU\n",
      "Job ID: f2e48d2e-3a0a-4a84-88dd-bcbc570e292e\n",
      "\n"
     ]
    },
    {
     "ename": "Forbidden",
     "evalue": "403 Access Denied: BigQuery BigQuery: Permission denied while getting Drive credentials.; reason: accessDenied, location: 1YJOR3XYHU3-KaxvI95bDgmVSeUjDXrEsTJnbOGYftE8, message: Access Denied: BigQuery BigQuery: Permission denied while getting Drive credentials.\n\nLocation: EU\nJob ID: 9c8c2df8-ae4c-4a2d-a4d6-3f35f32d9540\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mForbidden\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Create a native table in the same dataset\u001b[39;00m\n\u001b[32m     10\u001b[39m create_job = src_client.query(\n\u001b[32m     11\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[33m    CREATE OR REPLACE TABLE `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnative_table_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` AS\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[33m    SELECT * FROM `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc_table_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     15\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mcreate_job\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# wait for completion\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Now read from native table\u001b[39;00m\n\u001b[32m     19\u001b[39m df = src_client.query(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSELECT * FROM `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnative_table_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m).to_dataframe()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/google/cloud/bigquery/job/query.py:1706\u001b[39m, in \u001b[36mQueryJob.result\u001b[39m\u001b[34m(self, page_size, max_results, retry, timeout, start_index, job_retry)\u001b[39m\n\u001b[32m   1701\u001b[39m     remaining_timeout = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining_timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1704\u001b[39m     \u001b[38;5;66;03m# Since is_job_done() calls jobs.getQueryResults, which is a\u001b[39;00m\n\u001b[32m   1705\u001b[39m     \u001b[38;5;66;03m# long-running API, don't delay the next request at all.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1706\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_job_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1707\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   1708\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1709\u001b[39m     \u001b[38;5;66;03m# Use a monotonic clock since we don't actually care about\u001b[39;00m\n\u001b[32m   1710\u001b[39m     \u001b[38;5;66;03m# daylight savings or similar, just the elapsed time.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[32m    167\u001b[39m     time.sleep(next_sleep)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/google/api_core/retry/retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    216\u001b[39m     on_error_fn(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    149\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/google/cloud/bigquery/job/query.py:1655\u001b[39m, in \u001b[36mQueryJob.result.<locals>.is_job_done\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1632\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m job_failed_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1633\u001b[39m     \u001b[38;5;66;03m# Only try to restart the query job if the job failed for\u001b[39;00m\n\u001b[32m   1634\u001b[39m     \u001b[38;5;66;03m# a retriable reason. For example, don't restart the query\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1652\u001b[39m     \u001b[38;5;66;03m# into an exception that can be processed by the\u001b[39;00m\n\u001b[32m   1653\u001b[39m     \u001b[38;5;66;03m# `job_retry` predicate.\u001b[39;00m\n\u001b[32m   1654\u001b[39m     restart_query_job = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1655\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m job_failed_exception\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1657\u001b[39m     \u001b[38;5;66;03m# Make sure that the _query_results are cached so we\u001b[39;00m\n\u001b[32m   1658\u001b[39m     \u001b[38;5;66;03m# can return a complete RowIterator.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1664\u001b[39m     \u001b[38;5;66;03m# making any extra API calls if the previous loop\u001b[39;00m\n\u001b[32m   1665\u001b[39m     \u001b[38;5;66;03m# iteration fetched the finished job.\u001b[39;00m\n\u001b[32m   1666\u001b[39m     \u001b[38;5;28mself\u001b[39m._reload_query_results(\n\u001b[32m   1667\u001b[39m         retry=retry, **reload_query_results_kwargs\n\u001b[32m   1668\u001b[39m     )\n",
      "\u001b[31mForbidden\u001b[39m: 403 Access Denied: BigQuery BigQuery: Permission denied while getting Drive credentials.; reason: accessDenied, location: 1YJOR3XYHU3-KaxvI95bDgmVSeUjDXrEsTJnbOGYftE8, message: Access Denied: BigQuery BigQuery: Permission denied while getting Drive credentials.\n\nLocation: EU\nJob ID: 9c8c2df8-ae4c-4a2d-a4d6-3f35f32d9540\n"
     ]
    }
   ],
   "source": [
    "native_table_id = f\"{src_client.project}.course15.cc_funnel_native\"  # underscore\n",
    "\n",
    "try:\n",
    "    # Try to read the table (external table)\n",
    "    df = src_client.query(f\"SELECT * FROM `{src_table_id}`\").to_dataframe()\n",
    "except Exception as e:\n",
    "    print(\"External table blocked, creating native copy...\", e)\n",
    "    \n",
    "    # Create a native table in the same dataset\n",
    "    create_job = src_client.query(\n",
    "        f\"\"\"\n",
    "        CREATE OR REPLACE TABLE `{native_table_id}` AS\n",
    "        SELECT * FROM `{src_table_id}`\n",
    "        \"\"\"\n",
    "    )\n",
    "    create_job.result()  # wait for completion\n",
    "\n",
    "    # Now read from native table\n",
    "    df = src_client.query(f\"SELECT * FROM `{native_table_id}`\").to_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d324626-64ee-43dc-a2ee-f4b00088aff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1b/n00h51m115q53l5qwk1gc0m40000gn/T/ipykernel_49881/630193174.py:10: FutureWarning: read_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.read_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.read_gbq\n",
      "  df = pd.read_gbq(f\"SELECT * FROM `{src_table_id}`\",\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Please install the 'db-dtypes' package to use this function.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/google/cloud/bigquery/_pandas_helpers.py:59\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdb_dtypes\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     61\u001b[39m     date_dtype_name = db_dtypes.DateDtype.name\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'db_dtypes'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m src_table_id = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc_client.project\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.course15.cc_funnel_native\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Read table into DataFrame\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_gbq\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSELECT * FROM `\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msrc_table_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m`\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mproject_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_credentials\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pass source credentials\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Read \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows from source table\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/pandas/io/gbq.py:207\u001b[39m, in \u001b[36mread_gbq\u001b[39m\u001b[34m(query, project_id, index_col, col_order, reauth, auth_local_webserver, dialect, location, configuration, credentials, use_bqstorage_api, max_results, progress_bar_type)\u001b[39m\n\u001b[32m    204\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mprogress_bar_type\u001b[39m\u001b[33m\"\u001b[39m] = progress_bar_type\n\u001b[32m    205\u001b[39m \u001b[38;5;66;03m# END: new kwargs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_gbq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_gbq\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproject_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcol_order\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcol_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth_local_webserver\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth_local_webserver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/pandas_gbq/gbq.py:322\u001b[39m, in \u001b[36mread_gbq\u001b[39m\u001b[34m(query_or_table, project_id, index_col, columns, reauth, auth_local_webserver, dialect, location, configuration, credentials, use_bqstorage_api, max_results, verbose, private_key, progress_bar_type, dtypes, auth_redirect_uri, client_id, client_secret, col_order, bigquery_client)\u001b[39m\n\u001b[32m    306\u001b[39m connector = GbqConnector(\n\u001b[32m    307\u001b[39m     project_id,\n\u001b[32m    308\u001b[39m     reauth=reauth,\n\u001b[32m   (...)\u001b[39m\u001b[32m    318\u001b[39m     bigquery_client=bigquery_client,\n\u001b[32m    319\u001b[39m )\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_query(query_or_table):\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     final_df = \u001b[43mconnector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_or_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     final_df = connector.download_table(\n\u001b[32m    331\u001b[39m         query_or_table,\n\u001b[32m    332\u001b[39m         max_results=max_results,\n\u001b[32m    333\u001b[39m         progress_bar_type=progress_bar_type,\n\u001b[32m    334\u001b[39m         dtypes=dtypes,\n\u001b[32m    335\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/pandas_gbq/gbq_connector.py:263\u001b[39m, in \u001b[36mGbqConnector.run_query\u001b[39m\u001b[34m(self, query, max_results, progress_bar_type, **kwargs)\u001b[39m\n\u001b[32m    251\u001b[39m     rows_iter = pandas_gbq.query.query_and_wait(\n\u001b[32m    252\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    253\u001b[39m         \u001b[38;5;28mself\u001b[39m.client,\n\u001b[32m   (...)\u001b[39m\u001b[32m    259\u001b[39m         timeout_ms=timeout_ms,\n\u001b[32m    260\u001b[39m     )\n\u001b[32m    262\u001b[39m dtypes = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mdtypes\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download_results\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrows_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprogress_bar_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/pandas_gbq/gbq_connector.py:320\u001b[39m, in \u001b[36mGbqConnector._download_results\u001b[39m\u001b[34m(self, rows_iter, max_results, progress_bar_type, user_dtypes)\u001b[39m\n\u001b[32m    318\u001b[39m     conversion_dtypes = _bqschema_to_nullsafe_dtypes(schema_fields)\n\u001b[32m    319\u001b[39m     conversion_dtypes.update(user_dtypes)\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m     df = \u001b[43mrows_iter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconversion_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_bqstorage_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_bqstorage_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m.http_error \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m    326\u001b[39m     \u001b[38;5;28mself\u001b[39m.process_http_error(ex)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/google/cloud/bigquery/table.py:2597\u001b[39m, in \u001b[36mRowIterator.to_dataframe\u001b[39m\u001b[34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, geography_as_object, bool_dtype, int_dtype, float_dtype, string_dtype, date_dtype, datetime_dtype, time_dtype, timestamp_dtype, range_date_dtype, range_datetime_dtype, range_timestamp_dtype)\u001b[39m\n\u001b[32m   2362\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mto_dataframe\u001b[39m(\n\u001b[32m   2363\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2364\u001b[39m     bqstorage_client: Optional[\u001b[33m\"\u001b[39m\u001b[33mbigquery_storage.BigQueryReadClient\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2383\u001b[39m     ] = DefaultPandasDTypes.RANGE_TIMESTAMP_DTYPE,\n\u001b[32m   2384\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mpandas.DataFrame\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2385\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create a pandas DataFrame by loading all pages of a query.\u001b[39;00m\n\u001b[32m   2386\u001b[39m \n\u001b[32m   2387\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2595\u001b[39m \n\u001b[32m   2596\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2597\u001b[39m     \u001b[43m_pandas_helpers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mverify_pandas_imports\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2599\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m geography_as_object \u001b[38;5;129;01mand\u001b[39;00m shapely \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2600\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(_NO_SHAPELY_ERROR)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/dsa/lib/python3.13/site-packages/google/cloud/bigquery/_pandas_helpers.py:1117\u001b[39m, in \u001b[36mverify_pandas_imports\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1115\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(_NO_PANDAS_ERROR) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas_import_exception\u001b[39;00m\n\u001b[32m   1116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m db_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(_NO_DB_TYPES_ERROR) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdb_dtypes_import_exception\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Please install the 'db-dtypes' package to use this function."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Make sure you have the library\n",
    "# pip install pandas-gbq --upgrade\n",
    "\n",
    "# Fully-qualified source table\n",
    "src_table_id = f\"{src_client.project}.course15.cc_funnel_native\"\n",
    "\n",
    "# Read table into DataFrame\n",
    "df = pd.read_gbq(f\"SELECT * FROM `{src_table_id}`\",\n",
    "                 project_id=src_client.project,\n",
    "                 credentials=src_client._credentials)  # pass source credentials\n",
    "print(f\"✅ Read {len(df)} rows from source table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c8616170-4afc-4eed-ba14-03d33a793ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import db_dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "673f77b2-a50d-454e-9916-6d2f3f60ccf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Dataset data-analytics-ns-470609.test01_db already exists\n",
      "✅ Copied data-analytics-ns.course15.cc_parcel_kpi_formatted → data-analytics-ns-470609.test01_db.source_table_copy\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import Conflict\n",
    "\n",
    "# ---------- Destination client ----------\n",
    "dst_client = bigquery.Client(credentials=dst_creds, project=dst_creds.project_id)\n",
    "\n",
    "# ---------- Table IDs ----------\n",
    "src_table_id = \"data-analytics-ns.course15.cc_parcel_kpi_formatted\"  # fully-qualified source table\n",
    "dst_dataset_id = f\"{dst_client.project}.test01_db\"            # destination dataset\n",
    "dst_table_name = \"source_table_copy\"                          # destination table name\n",
    "dst_table_id = f\"{dst_dataset_id}.{dst_table_name}\"\n",
    "\n",
    "# ---------- Create destination dataset if it doesn't exist ----------\n",
    "try:\n",
    "    dataset = bigquery.Dataset(dst_dataset_id)\n",
    "    dataset.location = \"EU\"\n",
    "    dst_client.create_dataset(dataset)\n",
    "    print(f\"✅ Created dataset {dst_dataset_id}\")\n",
    "except Conflict:\n",
    "    print(f\"⚠️ Dataset {dst_dataset_id} already exists\")\n",
    "\n",
    "# ---------- Copy table ----------\n",
    "from google.cloud.bigquery import CopyJobConfig\n",
    "\n",
    "job_config = CopyJobConfig(\n",
    "    write_disposition=\"WRITE_TRUNCATE\"  # overwrite if table exists\n",
    ")\n",
    "\n",
    "copy_job = dst_client.copy_table(\n",
    "    sources=src_table_id,       # source table (fully-qualified)\n",
    "    destination=dst_table_id,   # destination table\n",
    "    location=\"EU\",              # dataset location\n",
    "    job_config=job_config\n",
    ")\n",
    "\n",
    "copy_job.result()  # wait for completion\n",
    "print(f\"✅ Copied {src_table_id} → {dst_table_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72e3c38-ce44-4c3c-9688-a203bcc1738c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dsa)",
   "language": "python",
   "name": "dsa-ipykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
